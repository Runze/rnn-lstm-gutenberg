{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n",
    "import re\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gutenberg_id</th>\n",
       "      <th>title</th>\n",
       "      <th>space_cnt</th>\n",
       "      <th>rn</th>\n",
       "      <th>n</th>\n",
       "      <th>grp</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105</td>\n",
       "      <td>Persuasion</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>1002</td>\n",
       "      <td>train</td>\n",
       "      <td>sir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105</td>\n",
       "      <td>Persuasion</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>1002</td>\n",
       "      <td>train</td>\n",
       "      <td>walter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105</td>\n",
       "      <td>Persuasion</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>1002</td>\n",
       "      <td>train</td>\n",
       "      <td>elliot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105</td>\n",
       "      <td>Persuasion</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>1002</td>\n",
       "      <td>train</td>\n",
       "      <td>_comma_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>Persuasion</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>1002</td>\n",
       "      <td>train</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gutenberg_id       title  space_cnt  rn     n    grp     word\n",
       "0           105  Persuasion        112   1  1002  train      sir\n",
       "1           105  Persuasion        112   1  1002  train   walter\n",
       "2           105  Persuasion        112   1  1002  train   elliot\n",
       "3           105  Persuasion        112   1  1002  train  _comma_\n",
       "4           105  Persuasion        112   1  1002  train       of"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('works_words.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38115"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create word-to-ID mapping\n",
    "counter = collections.Counter(data['word'])\n",
    "count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "words, _ = list(zip(*count_pairs))\n",
    "word_to_id = dict(zip(words, range(len(words))))\n",
    "id_to_word = dict(zip(range(len(words)), words))\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separate data into training, valid, and testing\n",
    "train_data = [word_to_id[w] for w in list(data['word'][data['grp'] == 'train'])]\n",
    "valid_data = [word_to_id[w] for w in list(data['word'][data['grp'] == 'valid'])]\n",
    "test_data = [word_to_id[w] for w in list(data['word'][data['grp'] == 'test'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1842988, 206542, 218819)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(valid_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The beginning of training data:\n",
      "sir walter elliot _comma_ of kellynch hall _comma_ in somersetshire _comma_ was a man who _comma_ for his own amusement _comma_ never took up any book but the baronetage _semicolon_ there he found occupation for an idle hour _comma_ and consolation in a distressed one _semicolon_ there his faculties were\n",
      "\n",
      "The beginning of valid data:\n",
      "_quote_ indeed _comma_ my dear mrs smith _comma_ i want none _comma_ _quote_ cried anne _period_ _quote_ you have asserted nothing contradictory to what mr elliot appeared to be some years ago _period_ this is all in confirmation _comma_ rather _comma_ of what we used to hear and believe _period_\n",
      "\n",
      "The beginning of test data:\n",
      "the interruption had been short _comma_ though severe _comma_ and ease and animation returned to most of those they left as the door shut them out _comma_ but not to anne _period_ she could think only of the invitation she had with such astonishment witnessed _comma_ and of the manner\n"
     ]
    }
   ],
   "source": [
    "print 'The beginning of training data:'\n",
    "print ' '.join([id_to_word[e] for e in train_data[:50]])\n",
    "\n",
    "print '\\nThe beginning of valid data:'\n",
    "print ' '.join([id_to_word[e] for e in valid_data[:50]])\n",
    "\n",
    "print '\\nThe beginning of test data:'\n",
    "print ' '.join([id_to_word[e] for e in test_data[:50]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Based on the PTB tutorial code.\n",
    "class config(object):\n",
    "    vocab_size = vocab_size\n",
    "    batch_size = 20\n",
    "    num_steps = 35  # sequence length; the number of unrolls\n",
    "    hidden_size = 256  # number of hidden units in LSTM; also embedding size\n",
    "    keep_prob = 0.5  # 1 - dropoff rate\n",
    "    num_layers = 2  # number of LSTM layers\n",
    "    max_grad_norm = 5  # max gradient (to prevent the exploding gradient problems)\n",
    "    init_scale = 0.05  # the initial scale of the weights\n",
    "    max_epoch = 6  # the number of epochs trained with the initial learning rate\n",
    "    max_max_epoch = 20  # the total number of epochs for training\n",
    "    learning_rate = 1.0  # the initial value of the learning rate\n",
    "    lr_decay = 0.8  # the decay of the learning rate for each epoch after \"max_epoch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_producer(raw_data, batch_size, num_steps, name=None):\n",
    "    \"\"\"\n",
    "    Iterate on the raw data.\n",
    "    \n",
    "    This chunks up raw_data into batches of examples and returns Tensors that\n",
    "    are drawn from these batches.\n",
    "    \n",
    "    Args:\n",
    "        raw_data: one of train_data, valid_data, and test_data.\n",
    "        batch_size: int, the batch size.\n",
    "        num_steps: int, the number of unrolls.\n",
    "        name: the name of this operation (optional).\n",
    "    Returns:\n",
    "        A pair of Tensors, each shaped [batch_size, num_steps]. The second element\n",
    "        of the tuple is the same data time-shifted to the right by one.\n",
    "    Raises:\n",
    "        tf.errors.InvalidArgumentError: if batch_size or num_steps are too high.\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.name_scope(name, \"BatchProducer\", [raw_data, batch_size, num_steps]):\n",
    "        raw_data = tf.convert_to_tensor(\n",
    "            raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "\n",
    "        data_len = tf.size(raw_data)\n",
    "        batch_len = data_len // batch_size\n",
    "        data = tf.reshape(raw_data[0:batch_size * batch_len],\n",
    "                          [batch_size, batch_len])\n",
    "\n",
    "        epoch_size = (batch_len - 1) // num_steps\n",
    "        assertion = tf.assert_positive(\n",
    "            epoch_size,\n",
    "            message=\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "        with tf.control_dependencies([assertion]):\n",
    "            epoch_size = tf.identity(epoch_size, name=\"epoch_size\")\n",
    "\n",
    "        i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "        \n",
    "        x = tf.strided_slice(data, [0, i * num_steps], [batch_size, (i + 1) * num_steps])\n",
    "        x.set_shape([batch_size, num_steps])\n",
    "        \n",
    "        y = tf.strided_slice(data, [0, i * num_steps + 1], [batch_size, (i + 1) * num_steps + 1])\n",
    "        y.set_shape([batch_size, num_steps])\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit hard to understand the code above without establishing a session, so I'm going to break it down with simple Numpy operations to see exactly what it's doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = config.batch_size\n",
    "num_steps = config.num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1842988, 92149)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use train_data as an example\n",
    "raw_data = train_data\n",
    "\n",
    "data_len = len(raw_data)\n",
    "batch_len = data_len // batch_size\n",
    "\n",
    "data_len, batch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 92149)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(raw_data[0:batch_size * batch_len]).reshape(batch_size, batch_len)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2632"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_size = (batch_len - 1) // num_steps\n",
    "epoch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 35), (20, 35))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "x = tf.strided_slice(data, [0, i * num_steps], [batch_size, (i + 1) * num_steps])\n",
    "y = tf.strided_slice(data, [0, i * num_steps + 1], [batch_size, (i + 1) * num_steps + 1])\n",
    "\n",
    "# Open an interactive session to evaluate generated tensors\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = x.eval()\n",
    "y = y.eval()\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.strided_slice is equivalent to slicing in Numpy:\n",
    "# https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html\n",
    "\n",
    "assert np.array_equal(x, data[0:batch_size:1, i * num_steps:(i + 1) * num_steps:1])\n",
    "assert np.array_equal(y, data[0:batch_size:1, i * num_steps + 1:(i + 1) * num_steps + 1:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 sir walter elliot _comma_ of kellynch hall _comma_ in somersetshire _comma_ was a man who _comma_ for his own amusement _comma_ never took up any book but the baronetage _semicolon_ there he found occupation for \n",
      "\n",
      "Batch 2 being also dancing _comma_ catherine was left to the mercy of mrs _period_ thorpe and mrs _period_ allen _comma_ between whom she now remained _period_ she could not help being vexed at the non _dash_ \n",
      "\n",
      "Batch 3 _semicolon_ for i assure you mr _period_ wingfield told me _comma_ that he did not believe he had ever sent us off altogether _comma_ in such good case _period_ i trust _comma_ at least _comma_ \n",
      "\n",
      "Batch 4 was wishing to get the better of his attachment to herself _comma_ she just recovering from her mania for mr _period_ elton _period_ it seemed as if every thing united to promise the most interesting \n",
      "\n",
      "Batch 5 that knowledge _period_ mrs _period_ jennings left them earlier than usual _semicolon_ for she could not be easy till the middletons and palmers were able to grieve as much as herself _semicolon_ and positively refusing \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print out some x and y as examples\n",
    "for i in range(0, 5):\n",
    "    print 'Batch', i+1, ' '.join([id_to_word[wid] for wid in x[i]]), '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 walter elliot _comma_ of kellynch hall _comma_ in somersetshire _comma_ was a man who _comma_ for his own amusement _comma_ never took up any book but the baronetage _semicolon_ there he found occupation for an \n",
      "\n",
      "Batch 2 also dancing _comma_ catherine was left to the mercy of mrs _period_ thorpe and mrs _period_ allen _comma_ between whom she now remained _period_ she could not help being vexed at the non _dash_ appearance \n",
      "\n",
      "Batch 3 for i assure you mr _period_ wingfield told me _comma_ that he did not believe he had ever sent us off altogether _comma_ in such good case _period_ i trust _comma_ at least _comma_ that \n",
      "\n",
      "Batch 4 wishing to get the better of his attachment to herself _comma_ she just recovering from her mania for mr _period_ elton _period_ it seemed as if every thing united to promise the most interesting consequences \n",
      "\n",
      "Batch 5 knowledge _period_ mrs _period_ jennings left them earlier than usual _semicolon_ for she could not be easy till the middletons and palmers were able to grieve as much as herself _semicolon_ and positively refusing elinor's \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    print 'Batch', i+1, ' '.join([id_to_word[wid] for wid in y[i]]), '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate batches and put it in a class (for easy access later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelInput(object):\n",
    "    def __init__(self, config, data, name=None):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "        self.input_data, self.targets = batch_producer(\n",
    "            data, batch_size, num_steps, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, is_training, config, input_=None):\n",
    "        batch_size = config.batch_size\n",
    "        num_steps = config.num_steps\n",
    "        hidden_size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "        \n",
    "        if input_ is not None:\n",
    "            # For normal training and validation, input data is pre-defined by class `ModelInput`\n",
    "            self._input = input_\n",
    "            self._input_data = input_.input_data\n",
    "            self._targets = input_.targets\n",
    "            \n",
    "        else:\n",
    "            # For text generations, input data is generated and fed on the fly\n",
    "            self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "            self._targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "\n",
    "        def lstm_cell():\n",
    "            # With the latest TensorFlow source code (as of Mar 27, 2017),\n",
    "            # the BasicLSTMCell will need a reuse parameter which is unfortunately not\n",
    "            # defined in TensorFlow 1.0. To maintain backwards compatibility, we add\n",
    "            # an argument check here:\n",
    "            \n",
    "            if 'reuse' in inspect.getargspec(\n",
    "                    tf.contrib.rnn.BasicLSTMCell.__init__).args:\n",
    "                return tf.contrib.rnn.BasicLSTMCell(\n",
    "                    hidden_size,\n",
    "                    forget_bias=0.0,\n",
    "                    state_is_tuple=True,\n",
    "                    reuse=tf.get_variable_scope().reuse)\n",
    "            else:\n",
    "                return tf.contrib.rnn.BasicLSTMCell(\n",
    "                    hidden_size,\n",
    "                    forget_bias=0.0,\n",
    "                    state_is_tuple=True)\n",
    "            \n",
    "            # Note because we set `state_is_tuple=True`, the states are 2-tuples of the `c_state` and `h_state`\n",
    "            # `c_state` is the cell state\n",
    "            # `h_state` is the hidden state\n",
    "            # See this SO thread: https://stackoverflow.com/questions/41789133/c-state-and-m-state-in-tensorflow-lstm\n",
    "    \n",
    "        attn_cell = lstm_cell\n",
    "\n",
    "        # Implement dropoff (for training only)\n",
    "        if is_training and config.keep_prob < 1:\n",
    "\n",
    "            def attn_cell():\n",
    "                return tf.contrib.rnn.DropoutWrapper(\n",
    "                    lstm_cell(), output_keep_prob=config.keep_prob)\n",
    "\n",
    "        # Stacking multiple LSTMs\n",
    "        attn_cells = [attn_cell() for _ in range(config.num_layers)]\n",
    "        stacked_lstm = tf.contrib.rnn.MultiRNNCell(attn_cells, state_is_tuple=True)\n",
    "        \n",
    "        # Initialize states with zeros\n",
    "        # `_initial_state` is a list of `num_layers` tensors\n",
    "        # Each is a tuple of (`c_state`, `h_state`),\n",
    "        # and both `c_state` and `h_state` are shaped [batch_size, hidden_size]\n",
    "        self._initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        # The word IDs will be embedded into a dense representation before feeding to the LSTM.\n",
    "        # This allows the model to efficiently represent the knowledge about particular words.\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\n",
    "                \"embedding\", [vocab_size, hidden_size], dtype=tf.float32)\n",
    "            input_embeddings = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "            # The shape of `input_embeddings` is [batch_size, num_steps, hidden_size]\n",
    "        \n",
    "        # Implement dropoff (for training only)\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            input_embeddings = tf.nn.dropout(input_embeddings, config.keep_prob)\n",
    "\n",
    "        # Simplified version of models/tutorials/rnn/rnn.py's rnn().\n",
    "        # This builds an unrolled LSTM for tutorial purposes only.\n",
    "        # In general, use the rnn() or state_saving_rnn() from rnn.py.\n",
    "        #\n",
    "        # The alternative version of the code below is:\n",
    "        #\n",
    "        # inputs = tf.unstack(inputs, num=num_steps, axis=1)\n",
    "        # outputs, state = tf.contrib.rnn.static_rnn(\n",
    "        #     cell, inputs, initial_state=self._initial_state)\n",
    "        \n",
    "        # Unroll LSTM loop\n",
    "        outputs = []\n",
    "        state = self._initial_state\n",
    "        \n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                \n",
    "                (cell_output, state) = stacked_lstm(input_embeddings[:, time_step, :], state)\n",
    "                outputs.append(cell_output)\n",
    "                # `outputs` is a list of `num_steps` tensors, each shaped [batch_size, hidden_size]\n",
    "        \n",
    "        # Resize the ouput into a [batch_size * num_steps, hidden_size] matrix.\n",
    "        # Note axis=1 in `tf.reshape` below because we want to group words together according to its original sequence\n",
    "        # in order to compare with `targets` to compute loss later.\n",
    "        output = tf.reshape(tf.stack(axis=1, values=outputs), [-1, hidden_size])\n",
    "        \n",
    "        # Compute logits\n",
    "        softmax_w = tf.get_variable(\n",
    "            \"softmax_w\", [hidden_size, vocab_size], dtype=tf.float32)\n",
    "        softmax_b = tf.get_variable(\n",
    "            \"softmax_b\", [vocab_size], dtype=tf.float32)\n",
    "        \n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        # The shape of `logits` =\n",
    "        # [batch_size * num_steps, hidden_size] x [hidden_size, vocab_size] + [vocab_size] =\n",
    "        # [batch_size * num_steps, vocab_size]\n",
    "        \n",
    "        # Sample based on the size of logits (used for text generation)\n",
    "        self._logits_sample = tf.multinomial(logits, 1)\n",
    "        \n",
    "        # Reshape logits to be 3-D tensor for sequence loss\n",
    "        logits = tf.reshape(logits, [batch_size, num_steps, vocab_size])\n",
    "\n",
    "        # Use the contrib sequence loss and average over the batches\n",
    "        # Source code: https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/seq2seq/python/ops/loss.py#L30\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,  # shape: [batch_size, num_steps, vocab_size]\n",
    "            self._targets,  # shape: [batch_size, num_steps]\n",
    "            tf.ones([batch_size, num_steps], dtype=tf.float32),  # weights (all set to 1 here)\n",
    "            average_across_timesteps=False,\n",
    "            average_across_batch=True)\n",
    "\n",
    "        # Update the cost variables\n",
    "        self._cost = cost = tf.reduce_sum(loss)\n",
    "        self._final_state = state\n",
    "\n",
    "        if not is_training:\n",
    "            return\n",
    "\n",
    "        # Optimizer\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        \n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(cost, tvars), config.max_grad_norm)\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
    "        self._train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars),\n",
    "            global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "\n",
    "        self._new_lr = tf.placeholder(\n",
    "            tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "        self._lr_update = tf.assign(self._lr, self._new_lr)\n",
    "        \n",
    "        \n",
    "    # Update learning rate\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(self._lr_update, feed_dict={self._new_lr: lr_value})\n",
    "    \n",
    "    # Properties to be called model training\n",
    "    @property\n",
    "    def input(self):\n",
    "        return self._input\n",
    "    \n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "    \n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op\n",
    "    \n",
    "    @property\n",
    "    def logits_sample(self):\n",
    "        return self._logits_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(session, model, eval_op=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Runs the model on the given data.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state)\n",
    "    fetches = {\n",
    "        \"cost\": model.cost,\n",
    "        \"final_state\": model.final_state,\n",
    "    }\n",
    "    \n",
    "    if eval_op is not None:\n",
    "        fetches[\"eval_op\"] = eval_op\n",
    "    \n",
    "    # Recall that epoch_size = (batch_len - 1) // num_steps.\n",
    "    for step in range(model.input.epoch_size):\n",
    "        feed_dict = {}\n",
    "        \n",
    "        # Recall that `_initial_state` is a list of `num_layers` tensors\n",
    "        # Each is a tuple of (`c_state`, `h_state`)\n",
    "        for i, (c, h) in enumerate(model.initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "        \n",
    "        vals = session.run(fetches, feed_dict)\n",
    "        \n",
    "        # Extract cost and final_state after the current step,\n",
    "        # which become the new initial cost and state for the next step\n",
    "        cost = vals[\"cost\"]\n",
    "        state = vals[\"final_state\"]\n",
    "        \n",
    "        # Compute average cost up to the current step\n",
    "        costs += cost\n",
    "        iters += model.input.num_steps\n",
    "\n",
    "        if verbose and step % (model.input.epoch_size // 10) == 10:\n",
    "            print(\"%.3f (raw step: %.0f) perplexity: %.3f speed: %.0f wps\" %\n",
    "                  (step * 1.0 / model.input.epoch_size,\n",
    "                   step,\n",
    "                   np.exp(costs / iters),\n",
    "                   iters * model.input.batch_size / (time.time() - start_time)))\n",
    "\n",
    "    return np.exp(costs / iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use various pronouns as the start of each sentence\n",
    "feeds = ['he', 'she', 'it', 'mr', 'mrs', 'miss']\n",
    "feeds = [word_to_id[w] for w in feeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define sentence length\n",
    "text_length = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text(session, model, feed, text_length):\n",
    "    # Note: this function is based on `run_epoch` defined above\n",
    "    \n",
    "    state = session.run(model.initial_state)\n",
    "    fetches = {\n",
    "        \"final_state\": model.final_state,\n",
    "        \"logit_sample\": model.logits_sample\n",
    "    }\n",
    "    \n",
    "    generated_text = [feed]\n",
    "    \n",
    "    for i in range(text_length):\n",
    "        feed_dict = {}\n",
    "        feed_dict[model.input_data] = feed\n",
    "        \n",
    "        for i, (c, h) in enumerate(model.initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "        \n",
    "        vals = session.run(fetches, feed_dict)\n",
    "        \n",
    "        # Extract final_state and sampled logit after the current step,\n",
    "        # which become the new initial state and feed for the next step\n",
    "        state = vals[\"final_state\"]\n",
    "        feed = vals[\"logit_sample\"]\n",
    "        \n",
    "        # Append generated text\n",
    "        generated_text.append(feed)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_punctuations(t):\n",
    "    t = re.sub(\" _comma_\", \",\", t)\n",
    "    t = re.sub(\" _period_\", \".\", t)\n",
    "    t = re.sub(\"_quote_\", \"\\\"\", t)\n",
    "    t = re.sub(\"_dash_\", \"-\", t)\n",
    "    t = re.sub(\" _semicolon_\", \";\", t)\n",
    "    t = re.sub(\" _exclamation_\", \"!\", t)\n",
    "    t = re.sub(\" _question_\", \"?\", t)\n",
    "    t = re.sub(\" _colon_\", \":\", t)\n",
    "    t = re.sub(\"_leftparenthesis_ \", \"(\", t)\n",
    "    t = re.sub(\" _rightparenthesis_\", \")\", t)\n",
    "    t = re.sub(\"_leftbracket_ \", \"[\", t)\n",
    "    t = re.sub(\" _rightbracket_\", \"]\", t)\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run all the things!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path = 'model_output' + '_' + datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load configurations for training and validation\n",
    "train_valid_config = config()\n",
    "\n",
    "# Modify configurations for test data and feed for text generations\n",
    "eval_config = config()\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List to store generated texts\n",
    "generated_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Model/global_step/sec: 0\n"
     ]
    }
   ],
   "source": [
    "# Redirect all output to a file\n",
    "# First, save the default output\n",
    "orig_stdout = sys.stdout\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "\n",
    "    with tf.name_scope(\"Train\"):\n",
    "        train_input = ModelInput(config=train_valid_config, data=train_data, name=\"TrainInput\")\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            m = Model(is_training=True, config=train_valid_config, input_=train_input)\n",
    "        tf.summary.scalar(\"Training_Loss\", m.cost)\n",
    "        tf.summary.scalar(\"Learning_Rate\", m.lr)\n",
    "\n",
    "    with tf.name_scope(\"Valid\"):\n",
    "        valid_input = ModelInput(config=train_valid_config, data=valid_data, name=\"ValidInput\")\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            mvalid = Model(is_training=False, config=train_valid_config, input_=valid_input)\n",
    "        tf.summary.scalar(\"Validation_Loss\", mvalid.cost)\n",
    "    \n",
    "    with tf.name_scope(\"Test\"):\n",
    "        test_input = ModelInput(config=eval_config, data=test_data, name=\"TestInput\")\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            mtest = Model(is_training=False, config=eval_config, input_=test_input)\n",
    "    \n",
    "    # For text generations\n",
    "    with tf.name_scope(\"Feed\"):\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            mfeed = Model(is_training=False, config=eval_config)\n",
    "            \n",
    "    sv = tf.train.Supervisor(logdir=save_path)\n",
    "    with sv.managed_session() as session:\n",
    "        for i in range(config.max_max_epoch):\n",
    "            # Redirect output to a file\n",
    "            log_file_path = 'log_file_' + str(i) + '.txt'\n",
    "            f = open(os.path.join(save_path, log_file_path), 'w')\n",
    "            sys.stdout = f\n",
    "            \n",
    "            # Update learning_rate if necessary\n",
    "            lr_decay = config.lr_decay**max(i + 1 - config.max_epoch, 0.0)\n",
    "            m.assign_lr(session, config.learning_rate * lr_decay)\n",
    "            print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "            \n",
    "            # Compute train and valid perplexity\n",
    "            train_perplexity = run_epoch(session, m, eval_op=m.train_op, verbose=True)\n",
    "            print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "            \n",
    "            valid_perplexity = run_epoch(session, mvalid)\n",
    "            print(\"Epoch: %d Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "            \n",
    "            # Generate text\n",
    "            print(\"Sample text generation:\\n\")\n",
    "            for feed in feeds:\n",
    "                generated_text = generate_text(session, mfeed, np.array(feed).reshape(1, 1), text_length)\n",
    "                generated_text = ' '.join([id_to_word[text[0, 0]] for text in generated_text])\n",
    "                generated_text = convert_punctuations(generated_text)\n",
    "                print generated_text, '\\n'\n",
    "                \n",
    "                generated_texts.append(generated_text)\n",
    "            \n",
    "            f.close()\n",
    "            \n",
    "        # Finally, compute test perplextiy\n",
    "        log_file_path = 'log_file_test_perplexity.txt'\n",
    "        f = open(os.path.join(save_path, log_file_path), 'w')\n",
    "        sys.stdout = f\n",
    "\n",
    "        test_perplexity = run_epoch(session, mtest)\n",
    "        print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "        \n",
    "        print(\"Saving model to %s.\" % save_path)\n",
    "        sv.saver.save(session, save_path, global_step=sv.global_step)\n",
    "        \n",
    "        f.close()\n",
    "\n",
    "# Restore default output\n",
    "sys.stdout = orig_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train perplexity: 88.017618522\n",
      "Valid perplexity: 97.6411531537\n",
      "Test perplexity: 100.0255776\n"
     ]
    }
   ],
   "source": [
    "# Print out final perplexities\n",
    "with sv.managed_session() as session:\n",
    "    print 'Train perplexity:', train_perplexity\n",
    "    print 'Valid perplexity:', valid_perplexity\n",
    "    print 'Test perplexity:', test_perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Generate final texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with \"he\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he has introduced to everybody herself. we may have himself with more governed beauty than in conversation - who scrap this attitude that little herself has been. but one animality though i am tempted to keep about it. would you come across the company? i declare what you think dorian. he has a rough wire, and giving me much noise instead of bread. it is one who to know it was what he had. you know my husband \" s lack of uneasiness that would rue. i have been all great exquisite ideas. but here! in fact, my services to your great youth is quite well correct. i shall find the separation in the being it to nothing. advice is a love, but no more, in fact, the name may cease if i have no chance about beholding for any body a genuine bit of great indifference, that can not be quite popular. you find nothing of your personal reflection if my comfortable; that other parties i call, for all the land of life, they are twenty series of remarks and old and ugly benevolence: listen, at all, what shall your account of it merely? do i remember it, i am not charmed and and common? little child and friendship you know the same thing that has implies this society of it. and in letters i have told you that over business is not so popular and mental. nothing matters, upon art, and through a great change if you will have our noble greatest stupid designs. but we like so well that happiness is simply coming to be kept on the room. only so well ago, he wrote it. you do not realise sibyl whibley no beauty that those who have failed to understand? at last i have explained you upon the same writer in a different hand, and where is necessarily ugly at your houses, before with the story of the great world and metaphysics, that this is my chief principle as it meant, and how sincerely i absolutely admit to you, the whole world one must be killed, and athens, who are a man of pain and the temper of a lull, has clothed the beauty of his life. i was all and wrong when to begin within one few ivory figures. we were at once seen at all, which, after mr. mahaffy, with the true secrets of shakespeare, for both type and epithet. in these volumes it is the true effect; how near the work will show; and by the artist he knows to me so, and to less can have loved st. hermann matthews, whose lives interest what is\n"
     ]
    }
   ],
   "source": [
    "feed = word_to_id['he']\n",
    "with sv.managed_session() as session:\n",
    "    generated_text = generate_text(session, mfeed, np.array(feed).reshape(1, 1), text_length)\n",
    "    generated_text = ' '.join([id_to_word[text[0, 0]] for text in generated_text])\n",
    "    generated_text = convert_punctuations(generated_text)\n",
    "\n",
    "    print generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with \"she\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she was a very pleasing girl. no doubt, but his wife has destroyed her secretary, which respect one most than she had not been rough. as if you were in the age of disliking life, mrs. bellamy, his dear lord arthur, we must have felt much as a second company upon the ungrateful and staffordshire; the most important som'n for the friendship of humanity, power, for all us a few thousand months protested, when no one cares in an invaluable, am always going on any professions of dealing with person by a certain subject of literature in the world. for christ, too, tuesday is mr. otis nowadays, as he was annoyed. nay, basil, this question has precedence to be found popular, for beginning to make a commonplace good good common view against love, she ought to produce an euphrates, and, though it is impossible, it is a capital meal in rain and secondly. it has nancy believed, whether all personalities were generally needed as his own manner. it is too late to give milton day to reply before the brawling change in which the intellect was placed with herself. it was not an imaginary madness to point out from the picture things by conversation, but in the early conception of that one, the great masterpieces of jealousy will share its tyranny away, in their stages of gladness. the true legion, the spirit of hand and the passion, may take from future. bookkeeper is too noble for his own, but the closest mood, a fresh element - fresh a vision to prevail on keble, crowded beauty, less than some else in one common health. that he is once trying to recognise himself; and nothing flashes from revellers, and so can. they are so kind, but combines each need. the time and the temper of its effect are not furies, and you must know what word is to suffer to fret, or only subtly well, in such sanguinely, that there is its hearts, you kissed us. we live there. that is a wretched costume of genius. the narrow limits: in course, in the beautiful - arched cliffs, they think how well i saw and as art is drops up, all others are beautiful. it is the gay summer as the iron macerated almond - tree, and not sometimes who have lost a little boy, when any judgment felt there. the seasons are crowded with white lips and dim butterflies. the emperies does not believe how. it does not appear more; and on interference of life, there is no literary pencil in whatever other horrible fiction:\n"
     ]
    }
   ],
   "source": [
    "feed = word_to_id['she']\n",
    "with sv.managed_session() as session:\n",
    "    generated_text = generate_text(session, mfeed, np.array(feed).reshape(1, 1), text_length)\n",
    "    generated_text = ' '.join([id_to_word[text[0, 0]] for text in generated_text])\n",
    "    generated_text = convert_punctuations(generated_text)\n",
    "\n",
    "    print generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with \"I\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have got patience at my feet, or a smile at all sort of moral life. their truth is written to me, as a delightful generalise, if i hope entirely didactic than we should have spent a men. \" she kane was apparently so pleasant. the vague and sobs of that travellers was blown on. the lawn ensued for supple ivory, and the rage of serpentlike transition on the cave, and are blown into the atmosphere of the past, and as a sun landed against the knout. in its ears, when they reached exploring of each side, again, the flowers stood and followed with languorously - sergeants the living, a designer of fleet parlor, swaying about, and before he stood at the club ball. the last morning, false, and crashing towards the temples, with his eyes went into a stray look. the servant and the three girls took an inch in the street. the servants as a star fell open from his lips. to the poor painter remarked. he asked his question, and it was swift and gay, and she had not seen mr. otis. this is written in anglo and pond, and was quite defiant to make their way back on the glen, and then with mrs. alan, who was annoyed, then wildly, having had an expression of amusement, the sucklings had done a walk to be worn in for the few maladies he had been placed with preferred good people. after any thing - neither could she say in the country. the voice was such a small master at a smile, a great mass of terror, and of his own muslin, continues over to see his counsel. he went on to a great loss of individualism, and had a terrible blush in inspiration in the road; of benjamin gray, who had long treated him by his birth with their own, would be, quite and all more little delicate, entirely more charming. she had ever watched them depended in engaging to make the utter creed of the people; phyllis browne had been shedding. he had a commonplace stage and made them four old distinctions - windows, and he was gratified by half a melodramas here, shot properly by the placard, as he complained to him that nowadays was every thing more truly pathetic than that he had been a combination of joy and dignity and his delight. she is popular and adventures, for their wedding with her letters of seeing her with tired, describing her most strongly as large as a beauty kensington - as a woman at all; and in the very latest drawing - book one had neither to make\n"
     ]
    }
   ],
   "source": [
    "feed = word_to_id['i']\n",
    "with sv.managed_session() as session:\n",
    "    generated_text = generate_text(session, mfeed, np.array(feed).reshape(1, 1), text_length)\n",
    "    generated_text = ' '.join([id_to_word[text[0, 0]] for text in generated_text])\n",
    "    generated_text = convert_punctuations(generated_text)\n",
    "\n",
    "    print generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with \"it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is a thing not to be done, or without your life. the last person who then is always duty to express himself. he merely gave me a far expressive of the deepest beauty of rage for all the inhabitants - always with their wonderful air of life; in life, with very pleasant aspect in the manner of the entirely subsequent valuable life: more than by some of the windows of twenty thousand passages when one should understand it to be one of the most splendid, yet really shelley is the most excellent musician by the first child. a most interesting hero it is in itself impossible to find higher manners than rip poetry to macmillan's prose. the passage as a new cure brings us up all these scroll is moral, and is form in congealed. every cultivation of songs who fills life by portman leaf we are keenly interested in existence that is inevitably still worth reading. it does not try to have or been guilty of having a pleasure; but perhaps not there is any real form that seems to have lost its secrets when at first, and the lines of a great field at least of the cups of fielding, mocking rapidly of a god and almost contain the effect of it, i apprehend it on side to take their divine subject; count metre, the feeble - leaves that have while often forfeited to the philosopher, education is intensely moved, while the diminutive heart of marco lamb suggests that the type of a man has seen a drink, amid the sunken station of the masses which was bent out at first. and so sadly still etchings by felicitated in elizabethan swiftness, as a writer of some great considerations in whose great character would have ever been great to be beforehand, but though through the senses for her contrasted an earl of delicacy he might have been paramount and the same people found the convents, for we once felt that they might break out the beautiful era by the apocalyptic laughter of patterns, the system at florence, to that high city of greek flight, too, and no real pageant, much aim. the wheedled of leuctra and isabel she president untended their conqueror, that was the first time that he hated balcony in fine eve, and in the present grace of action, while the season of noble shuns and lovers varied as beautiful as a story of the tower and athens of balance, and east whose violet jaw, and coos - coloured and uncouth. when work has been given, its ardent spirit seemed never scabbard. the discoverers of the book stands no longer to the fair and extinguished fringes, nor sought want with the sting of furnishing concentration; but\n"
     ]
    }
   ],
   "source": [
    "feed = word_to_id['it']\n",
    "with sv.managed_session() as session:\n",
    "    generated_text = generate_text(session, mfeed, np.array(feed).reshape(1, 1), text_length)\n",
    "    generated_text = ' '.join([id_to_word[text[0, 0]] for text in generated_text])\n",
    "    generated_text = convert_punctuations(generated_text)\n",
    "\n",
    "    print generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration of model restore after kernel reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored from file: model_output_2017-07-29-07-09/model.ckpt\n",
      "\n",
      "he came along the streets of a sight. who were not mounteagle or a gentleman or an inconsiderable man? the fact a man should know to be the fact. he was something rather proportioned, and when the sordid intellect appeared away and understanding, as the probable duke of berwick it felt, and, while twice after the time, the curious stars came the magic from the road, the darkness was his thorax, and talisman, and not aware that the ravens were not a real moon, as he had the place of a scholar, a decent tourist chant, note thorn and coarse looks seemed rather inflicting its cunning or more. he looked in, there was only a most subtle and automatics character in the corner; and the panes of the sea ran her cheeks at the wheel, and said none of his pictures. \" you know what i do just do me first thus your servant have done, \" answered edward, feeling some pain and colour. \" she is a sorry creature, \" said lord henry. \" you have been the same niece, \" said the fisherman. \" i want to understand i am so entertaining that she is lying in - flowers of them. after the time, of course, you always canonise and indeed for your last time to express them. they have never any contrivance or rejecting my perfection. if, after heaven's sake, you will have her husband and attention herself. if i know me i beleive you always have got a life to me. nobody here is at them. i suppose it is which this artist is going to be revolting by a certain problem, who then always assiduous appetite. on the subject of the influence of god, i am afraid that he has not spoken in a deity, it would be a wonderful source of human success. the highest care of the nation, in accordance with the real and all faults of pythagorean service; a natural imagination, absolutely completely indispensably a very unfit to have glowing the beauties of the work of the genius. you have tried to see nothing but any good shape about the new instinct of art i sought to approach you. even it is the precursor of that epithets for the public.. hearing me a few days. practical vulgarity. nobody i have found; it is both in the way my father. after all, i have spoken into the simple sir in the course of the abstract academy when you are not archway you have to live about the novel at all. you must have thought that married art bulwer and harriet in its minor francisco have happened at another. deck\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "    \n",
    "    # Define model for text generations\n",
    "    with tf.name_scope(\"Feed\"):\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            mfeed = Model(is_training=False, config=eval_config)\n",
    "    \n",
    "    sv = tf.train.Supervisor(logdir=save_path)\n",
    "    with sv.managed_session() as session:\n",
    "        # Restore model weights from previously saved model\n",
    "        ckpt = tf.train.get_checkpoint_state(save_path)\n",
    "        sv.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "        print(\"Model restored from file: %s\\n\" % ckpt.model_checkpoint_path)\n",
    "        \n",
    "        # Now that the model is restored, we can generate texts again\n",
    "        generated_text = generate_text(session, mfeed, np.array(feeds[0]).reshape(1, 1), text_length)\n",
    "        generated_text = ' '.join([id_to_word[text[0, 0]] for text in generated_text])\n",
    "        generated_text = convert_punctuations(generated_text)\n",
    "        \n",
    "        print generated_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 (General DS)",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
