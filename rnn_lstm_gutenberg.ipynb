{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n",
    "import re\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gutenberg_id</th>\n",
       "      <th>title</th>\n",
       "      <th>space_cnt</th>\n",
       "      <th>rn</th>\n",
       "      <th>n</th>\n",
       "      <th>grp</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105</td>\n",
       "      <td>Persuasion</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>1002</td>\n",
       "      <td>train</td>\n",
       "      <td>sir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105</td>\n",
       "      <td>Persuasion</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>1002</td>\n",
       "      <td>train</td>\n",
       "      <td>walter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105</td>\n",
       "      <td>Persuasion</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>1002</td>\n",
       "      <td>train</td>\n",
       "      <td>elliot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105</td>\n",
       "      <td>Persuasion</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>1002</td>\n",
       "      <td>train</td>\n",
       "      <td>_comma_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>Persuasion</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>1002</td>\n",
       "      <td>train</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gutenberg_id       title  space_cnt  rn     n    grp     word\n",
       "0           105  Persuasion        112   1  1002  train      sir\n",
       "1           105  Persuasion        112   1  1002  train   walter\n",
       "2           105  Persuasion        112   1  1002  train   elliot\n",
       "3           105  Persuasion        112   1  1002  train  _comma_\n",
       "4           105  Persuasion        112   1  1002  train       of"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('works_words.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38115"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create word-to-ID mapping\n",
    "counter = collections.Counter(data['word'])\n",
    "count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "words, _ = list(zip(*count_pairs))\n",
    "word_to_id = dict(zip(words, range(len(words))))\n",
    "id_to_word = dict(zip(range(len(words)), words))\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separate data into training, valid, and testing\n",
    "train_data = [word_to_id[w] for w in list(data['word'][data['grp'] == 'train'])]\n",
    "valid_data = [word_to_id[w] for w in list(data['word'][data['grp'] == 'valid'])]\n",
    "test_data = [word_to_id[w] for w in list(data['word'][data['grp'] == 'test'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1842988, 206542, 218819)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(valid_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The beginning of training data:\n",
      "sir walter elliot _comma_ of kellynch hall _comma_ in somersetshire _comma_ was a man who _comma_ for his own amusement _comma_ never took up any book but the baronetage _semicolon_ there he found occupation for an idle hour _comma_ and consolation in a distressed one _semicolon_ there his faculties were\n",
      "\n",
      "The beginning of valid data:\n",
      "_quote_ indeed _comma_ my dear mrs smith _comma_ i want none _comma_ _quote_ cried anne _period_ _quote_ you have asserted nothing contradictory to what mr elliot appeared to be some years ago _period_ this is all in confirmation _comma_ rather _comma_ of what we used to hear and believe _period_\n",
      "\n",
      "The beginning of test data:\n",
      "the interruption had been short _comma_ though severe _comma_ and ease and animation returned to most of those they left as the door shut them out _comma_ but not to anne _period_ she could think only of the invitation she had with such astonishment witnessed _comma_ and of the manner\n"
     ]
    }
   ],
   "source": [
    "print 'The beginning of training data:'\n",
    "print ' '.join([id_to_word[e] for e in train_data[:50]])\n",
    "\n",
    "print '\\nThe beginning of valid data:'\n",
    "print ' '.join([id_to_word[e] for e in valid_data[:50]])\n",
    "\n",
    "print '\\nThe beginning of test data:'\n",
    "print ' '.join([id_to_word[e] for e in test_data[:50]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class config(object):\n",
    "    vocab_size = vocab_size\n",
    "    batch_size = 20\n",
    "    num_steps = 35  # sequence length; the number of unrolls\n",
    "    hidden_size = 300  # number of hidden units in LSTM; also embedding size\n",
    "    keep_prob = 0.5  # 1 - dropoff rate\n",
    "    num_layers = 2  # number of LSTM layers\n",
    "    max_grad_norm = 5  # max gradient (to prevent the exploding gradient problems)\n",
    "    init_scale = 0.05  # the initial scale of the weights\n",
    "    max_epoch = 4  # the number of epochs trained with the initial learning rate\n",
    "    max_max_epoch = 20  # the total number of epochs for training\n",
    "    learning_rate = 1.0  # the initial value of the learning rate\n",
    "    lr_decay = 0.5  # the decay of the learning rate for each epoch after \"max_epoch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_producer(raw_data, batch_size, num_steps, name=None):\n",
    "    \"\"\"\n",
    "    Iterate on the raw data.\n",
    "    \n",
    "    This chunks up raw_data into batches of examples and returns Tensors that\n",
    "    are drawn from these batches.\n",
    "    \n",
    "    Args:\n",
    "        raw_data: one of train_data, valid_data, and test_data.\n",
    "        batch_size: int, the batch size.\n",
    "        num_steps: int, the number of unrolls.\n",
    "        name: the name of this operation (optional).\n",
    "    Returns:\n",
    "        A pair of Tensors, each shaped [batch_size, num_steps]. The second element\n",
    "        of the tuple is the same data time-shifted to the right by one.\n",
    "    Raises:\n",
    "        tf.errors.InvalidArgumentError: if batch_size or num_steps are too high.\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.name_scope(name, \"BatchProducer\", [raw_data, batch_size, num_steps]):\n",
    "        raw_data = tf.convert_to_tensor(\n",
    "            raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "\n",
    "        data_len = tf.size(raw_data)\n",
    "        batch_len = data_len // batch_size\n",
    "        data = tf.reshape(raw_data[0:batch_size * batch_len],\n",
    "                          [batch_size, batch_len])\n",
    "\n",
    "        epoch_size = (batch_len - 1) // num_steps\n",
    "        assertion = tf.assert_positive(\n",
    "            epoch_size,\n",
    "            message=\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "        with tf.control_dependencies([assertion]):\n",
    "            epoch_size = tf.identity(epoch_size, name=\"epoch_size\")\n",
    "\n",
    "        i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "        \n",
    "        x = tf.strided_slice(data, [0, i * num_steps], [batch_size, (i + 1) * num_steps])\n",
    "        x.set_shape([batch_size, num_steps])\n",
    "        \n",
    "        y = tf.strided_slice(data, [0, i * num_steps + 1], [batch_size, (i + 1) * num_steps + 1])\n",
    "        y.set_shape([batch_size, num_steps])\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit hard to understand the code above without establishing a session, so I'm going to break it down with simple Numpy operations to see exactly what it's doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = config.batch_size\n",
    "num_steps = config.num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1842988, 92149)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use train_data as an example\n",
    "raw_data = train_data\n",
    "\n",
    "data_len = len(raw_data)\n",
    "batch_len = data_len // batch_size\n",
    "\n",
    "data_len, batch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 92149)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(raw_data[0:batch_size * batch_len]).reshape(batch_size, batch_len)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2632"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_size = (batch_len - 1) // num_steps\n",
    "epoch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 35), (20, 35))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "x = tf.strided_slice(data, [0, i * num_steps], [batch_size, (i + 1) * num_steps])\n",
    "y = tf.strided_slice(data, [0, i * num_steps + 1], [batch_size, (i + 1) * num_steps + 1])\n",
    "\n",
    "# Open an interactive session to evaluate generated tensors\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = x.eval()\n",
    "y = y.eval()\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.strided_slice is equivalent to slicing in Numpy:\n",
    "# https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html\n",
    "\n",
    "assert np.array_equal(x, data[0:batch_size:1, i * num_steps:(i + 1) * num_steps:1])\n",
    "assert np.array_equal(y, data[0:batch_size:1, i * num_steps + 1:(i + 1) * num_steps + 1:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 sir walter elliot _comma_ of kellynch hall _comma_ in somersetshire _comma_ was a man who _comma_ for his own amusement _comma_ never took up any book but the baronetage _semicolon_ there he found occupation for \n",
      "\n",
      "Batch 2 being also dancing _comma_ catherine was left to the mercy of mrs _period_ thorpe and mrs _period_ allen _comma_ between whom she now remained _period_ she could not help being vexed at the non _dash_ \n",
      "\n",
      "Batch 3 _semicolon_ for i assure you mr _period_ wingfield told me _comma_ that he did not believe he had ever sent us off altogether _comma_ in such good case _period_ i trust _comma_ at least _comma_ \n",
      "\n",
      "Batch 4 was wishing to get the better of his attachment to herself _comma_ she just recovering from her mania for mr _period_ elton _period_ it seemed as if every thing united to promise the most interesting \n",
      "\n",
      "Batch 5 that knowledge _period_ mrs _period_ jennings left them earlier than usual _semicolon_ for she could not be easy till the middletons and palmers were able to grieve as much as herself _semicolon_ and positively refusing \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print out some x and y as examples\n",
    "for i in range(0, 5):\n",
    "    print 'Batch', i+1, ' '.join([id_to_word[wid] for wid in x[i]]), '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 walter elliot _comma_ of kellynch hall _comma_ in somersetshire _comma_ was a man who _comma_ for his own amusement _comma_ never took up any book but the baronetage _semicolon_ there he found occupation for an \n",
      "\n",
      "Batch 2 also dancing _comma_ catherine was left to the mercy of mrs _period_ thorpe and mrs _period_ allen _comma_ between whom she now remained _period_ she could not help being vexed at the non _dash_ appearance \n",
      "\n",
      "Batch 3 for i assure you mr _period_ wingfield told me _comma_ that he did not believe he had ever sent us off altogether _comma_ in such good case _period_ i trust _comma_ at least _comma_ that \n",
      "\n",
      "Batch 4 wishing to get the better of his attachment to herself _comma_ she just recovering from her mania for mr _period_ elton _period_ it seemed as if every thing united to promise the most interesting consequences \n",
      "\n",
      "Batch 5 knowledge _period_ mrs _period_ jennings left them earlier than usual _semicolon_ for she could not be easy till the middletons and palmers were able to grieve as much as herself _semicolon_ and positively refusing elinor's \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    print 'Batch', i+1, ' '.join([id_to_word[wid] for wid in y[i]]), '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate batches and put it in a class (for easy access later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelInput(object):\n",
    "    def __init__(self, config, data, name=None):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "        self.input_data, self.targets = batch_producer(\n",
    "            data, batch_size, num_steps, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, is_training, config, input_=None):\n",
    "        batch_size = config.batch_size\n",
    "        num_steps = config.num_steps\n",
    "        hidden_size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "        \n",
    "        if input_ is not None:\n",
    "            # For normal training and validation, input data is pre-defined by class `ModelInput`\n",
    "            self._input = input_\n",
    "            self._input_data = input_.input_data\n",
    "            self._targets = input_.targets\n",
    "            \n",
    "        else:\n",
    "            # For text generations, input data is generated and fed on the fly\n",
    "            self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "            self._targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "\n",
    "        def lstm_cell():\n",
    "            # With the latest TensorFlow source code (as of Mar 27, 2017),\n",
    "            # the BasicLSTMCell will need a reuse parameter which is unfortunately not\n",
    "            # defined in TensorFlow 1.0. To maintain backwards compatibility, we add\n",
    "            # an argument check here:\n",
    "            \n",
    "            if 'reuse' in inspect.getargspec(\n",
    "                    tf.contrib.rnn.BasicLSTMCell.__init__).args:\n",
    "                return tf.contrib.rnn.BasicLSTMCell(\n",
    "                    hidden_size,\n",
    "                    forget_bias=0.0,\n",
    "                    state_is_tuple=True,\n",
    "                    reuse=tf.get_variable_scope().reuse)\n",
    "            else:\n",
    "                return tf.contrib.rnn.BasicLSTMCell(\n",
    "                    hidden_size,\n",
    "                    forget_bias=0.0,\n",
    "                    state_is_tuple=True)\n",
    "            \n",
    "            # Note because we set `state_is_tuple=True`, the states are 2-tuples of the `c_state` and `h_state`\n",
    "            # `c_state` is the cell state\n",
    "            # `h_state` is the hidden state\n",
    "            # See this SO thread: https://stackoverflow.com/questions/41789133/c-state-and-m-state-in-tensorflow-lstm\n",
    "    \n",
    "        attn_cell = lstm_cell\n",
    "\n",
    "        # Implement dropoff (for training only)\n",
    "        if is_training and config.keep_prob < 1:\n",
    "\n",
    "            def attn_cell():\n",
    "                return tf.contrib.rnn.DropoutWrapper(\n",
    "                    lstm_cell(), output_keep_prob=config.keep_prob)\n",
    "\n",
    "        # Stacking multiple LSTMs\n",
    "        attn_cells = [attn_cell() for _ in range(config.num_layers)]\n",
    "        stacked_lstm = tf.contrib.rnn.MultiRNNCell(attn_cells, state_is_tuple=True)\n",
    "        \n",
    "        # Initialize states with zeros\n",
    "        # `_initial_state` is a list of `num_layers` tensors\n",
    "        # Each is a tuple of (`c_state`, `h_state`),\n",
    "        # and both `c_state` and `h_state` are shaped [batch_size, hidden_size]\n",
    "        self._initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        # The word IDs will be embedded into a dense representation before feeding to the LSTM.\n",
    "        # This allows the model to efficiently represent the knowledge about particular words.\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\n",
    "                \"embedding\", [vocab_size, hidden_size], dtype=tf.float32)\n",
    "            input_embeddings = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "            # The shape of `input_embeddings` is [batch_size, num_steps, hidden_size]\n",
    "        \n",
    "        # Implement dropoff (for training only)\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            input_embeddings = tf.nn.dropout(input_embeddings, config.keep_prob)\n",
    "\n",
    "        # Simplified version of models/tutorials/rnn/rnn.py's rnn().\n",
    "        # This builds an unrolled LSTM for tutorial purposes only.\n",
    "        # In general, use the rnn() or state_saving_rnn() from rnn.py.\n",
    "        #\n",
    "        # The alternative version of the code below is:\n",
    "        #\n",
    "        # inputs = tf.unstack(inputs, num=num_steps, axis=1)\n",
    "        # outputs, state = tf.contrib.rnn.static_rnn(\n",
    "        #     cell, inputs, initial_state=self._initial_state)\n",
    "        \n",
    "        # Unroll LSTM loop\n",
    "        outputs = []\n",
    "        state = self._initial_state\n",
    "        \n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                \n",
    "                (cell_output, state) = stacked_lstm(input_embeddings[:, time_step, :], state)\n",
    "                outputs.append(cell_output)\n",
    "                # `outputs` is a list of `num_steps` tensors, each shaped [batch_size, hidden_size]\n",
    "        \n",
    "        # Resize the ouput into a [batch_size * num_steps, hidden_size] matrix.\n",
    "        # Note axis=1 in `tf.reshape` below because we want to group words together according to its original sequence\n",
    "        # in order to compare with `targets` to compute loss later.\n",
    "        output = tf.reshape(tf.stack(axis=1, values=outputs), [-1, hidden_size])\n",
    "        \n",
    "        # Compute logits\n",
    "        softmax_w = tf.get_variable(\n",
    "            \"softmax_w\", [hidden_size, vocab_size], dtype=tf.float32)\n",
    "        softmax_b = tf.get_variable(\n",
    "            \"softmax_b\", [vocab_size], dtype=tf.float32)\n",
    "        \n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        # The shape of `logits` =\n",
    "        # [batch_size * num_steps, hidden_size] x [hidden_size, vocab_size] + [vocab_size] =\n",
    "        # [batch_size * num_steps, vocab_size]\n",
    "        \n",
    "        # Sample based on the size of logits (used for text generation)\n",
    "        self._logits_sample = tf.multinomial(logits, 1)\n",
    "        \n",
    "        # Reshape logits to be 3-D tensor for sequence loss\n",
    "        logits = tf.reshape(logits, [batch_size, num_steps, vocab_size])\n",
    "\n",
    "        # Use the contrib sequence loss and average over the batches\n",
    "        # Source code: https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/seq2seq/python/ops/loss.py#L30\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,  # shape: [batch_size, num_steps, vocab_size]\n",
    "            self._targets,  # shape: [batch_size, num_steps]\n",
    "            tf.ones([batch_size, num_steps], dtype=tf.float32),  # weights (all set to 1 here)\n",
    "            average_across_timesteps=False,\n",
    "            average_across_batch=True)\n",
    "\n",
    "        # Update the cost variables\n",
    "        self._cost = cost = tf.reduce_sum(loss)\n",
    "        self._final_state = state\n",
    "\n",
    "        if not is_training:\n",
    "            return\n",
    "\n",
    "        # Optimizer\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        \n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(cost, tvars), config.max_grad_norm)\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
    "        self._train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars),\n",
    "            global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "\n",
    "        self._new_lr = tf.placeholder(\n",
    "            tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "        self._lr_update = tf.assign(self._lr, self._new_lr)\n",
    "        \n",
    "        \n",
    "    # Update learning rate\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(self._lr_update, feed_dict={self._new_lr: lr_value})\n",
    "    \n",
    "    # Properties to be called model training\n",
    "    @property\n",
    "    def input(self):\n",
    "        return self._input\n",
    "    \n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "    \n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op\n",
    "    \n",
    "    @property\n",
    "    def logits_sample(self):\n",
    "        return self._logits_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(session, model, eval_op=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Runs the model on the given data.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state)\n",
    "    fetches = {\n",
    "        \"cost\": model.cost,\n",
    "        \"final_state\": model.final_state,\n",
    "    }\n",
    "    \n",
    "    if eval_op is not None:\n",
    "        fetches[\"eval_op\"] = eval_op\n",
    "    \n",
    "    # Recall that epoch_size = (batch_len - 1) // num_steps.\n",
    "    for step in range(model.input.epoch_size):\n",
    "        feed_dict = {}\n",
    "        \n",
    "        # Recall that `_initial_state` is a list of `num_layers` tensors\n",
    "        # Each is a tuple of (`c_state`, `h_state`)\n",
    "        for i, (c, h) in enumerate(model.initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "        \n",
    "        vals = session.run(fetches, feed_dict)\n",
    "        \n",
    "        # Extract cost and final_state after the current step,\n",
    "        # which become the new initial cost and state for the next step\n",
    "        cost = vals[\"cost\"]\n",
    "        state = vals[\"final_state\"]\n",
    "        \n",
    "        # Compute average cost up to the current step\n",
    "        costs += cost\n",
    "        iters += model.input.num_steps\n",
    "\n",
    "        if verbose and step % (model.input.epoch_size // 10) == 10:\n",
    "            print(\"%.3f (raw step: %.0f) perplexity: %.3f speed: %.0f wps\" %\n",
    "                  (step * 1.0 / model.input.epoch_size,\n",
    "                   step,\n",
    "                   np.exp(costs / iters),\n",
    "                   iters * model.input.batch_size / (time.time() - start_time)))\n",
    "\n",
    "    return np.exp(costs / iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use various pronouns as the start of each sentence\n",
    "feeds = ['he', 'she', 'it', 'mr', 'mrs', 'miss']\n",
    "feeds = [word_to_id[w] for w in feeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define sentence length\n",
    "text_length = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text(session, model, feed, text_length):\n",
    "    # Note: this function is based on `run_epoch` defined above\n",
    "    \n",
    "    state = session.run(model.initial_state)\n",
    "    fetches = {\n",
    "        \"final_state\": model.final_state,\n",
    "        \"logit_sample\": model.logits_sample\n",
    "    }\n",
    "    \n",
    "    generated_text = [feed]\n",
    "    \n",
    "    for i in range(text_length):\n",
    "        feed_dict = {}\n",
    "        feed_dict[model.input_data] = feed\n",
    "        \n",
    "        for i, (c, h) in enumerate(model.initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "        \n",
    "        vals = session.run(fetches, feed_dict)\n",
    "        \n",
    "        # Extract final_state and sampled logit after the current step,\n",
    "        # which become the new initial state and feed for the next step\n",
    "        state = vals[\"final_state\"]\n",
    "        feed = vals[\"logit_sample\"]\n",
    "        \n",
    "        # Append generated text\n",
    "        generated_text.append(feed)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_punctuations(t):\n",
    "    t = re.sub(\" _comma_\", \",\", t)\n",
    "    t = re.sub(\" _period_\", \".\", t)\n",
    "    t = re.sub(\"_quote_\", \"\\\"\", t)\n",
    "    t = re.sub(\"_dash_\", \"-\", t)\n",
    "    t = re.sub(\" _semicolon_\", \";\", t)\n",
    "    t = re.sub(\" _exclamation_\", \"!\", t)\n",
    "    t = re.sub(\" _question_\", \"?\", t)\n",
    "    t = re.sub(\" _colon_\", \":\", t)\n",
    "    t = re.sub(\"_leftparenthesis_ \", \"(\", t)\n",
    "    t = re.sub(\" _rightparenthesis_\", \")\", t)\n",
    "    t = re.sub(\"_leftbracket_ \", \"[\", t)\n",
    "    t = re.sub(\" _rightbracket_\", \"]\", t)\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run all the things!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path = 'model_output' + '_' + datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load configurations for training and validation\n",
    "train_valid_config = config()\n",
    "\n",
    "# Modify configurations for test data and feed for text generations\n",
    "eval_config = config()\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List to store generated texts\n",
    "generated_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Redirect all output to a file\n",
    "# First, save the default output\n",
    "orig_stdout = sys.stdout\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "\n",
    "    with tf.name_scope(\"Train\"):\n",
    "        train_input = ModelInput(config=train_valid_config, data=train_data, name=\"TrainInput\")\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            m = Model(is_training=True, config=config, input_=train_input)\n",
    "        tf.summary.scalar(\"Training_Loss\", m.cost)\n",
    "        tf.summary.scalar(\"Learning_Rate\", m.lr)\n",
    "\n",
    "    with tf.name_scope(\"Valid\"):\n",
    "        valid_input = ModelInput(config=train_valid_config, data=valid_data, name=\"ValidInput\")\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            mvalid = Model(is_training=False, config=config, input_=valid_input)\n",
    "        tf.summary.scalar(\"Validation_Loss\", mvalid.cost)\n",
    "    \n",
    "    with tf.name_scope(\"Test\"):\n",
    "        test_input = ModelInput(config=eval_config, data=test_data, name=\"TestInput\")\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            mtest = Model(is_training=False, config=eval_config, input_=test_input)\n",
    "    \n",
    "    # For text generations\n",
    "    with tf.name_scope(\"Feed\"):\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            mfeed = Model(is_training=False, config=eval_config)\n",
    "            \n",
    "    sv = tf.train.Supervisor(logdir=save_path)\n",
    "    with sv.managed_session() as session:\n",
    "        for i in range(config.max_max_epoch):\n",
    "            # Redirect output to a file\n",
    "            log_file_path = 'log_file_' + str(i) + '.txt'\n",
    "            f = open(os.path.join(save_path, log_file_path), 'w')\n",
    "            sys.stdout = f\n",
    "            \n",
    "            # Update learning_rate if necessary\n",
    "            lr_decay = config.lr_decay**max(i + 1 - config.max_epoch, 0.0)\n",
    "            m.assign_lr(session, config.learning_rate * lr_decay)\n",
    "            print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "            \n",
    "            # Compute train and valid perplexity\n",
    "            train_perplexity = run_epoch(session, m, eval_op=m.train_op, verbose=True)\n",
    "            print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "            \n",
    "            valid_perplexity = run_epoch(session, mvalid)\n",
    "            print(\"Epoch: %d Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "            \n",
    "            # Generate text\n",
    "            print(\"Sample text generation:\\n\")\n",
    "            for feed in feeds:\n",
    "                generated_text = generate_text(session, mfeed, np.array(feed).reshape(1, 1), text_length)\n",
    "                generated_text = ' '.join([id_to_word[text[0, 0]] for text in generated_text])\n",
    "                generated_text = convert_punctuations(generated_text)\n",
    "                print generated_text, '\\n'\n",
    "                \n",
    "                generated_texts.append(generated_text)\n",
    "            \n",
    "            f.close()\n",
    "            \n",
    "        # Finally, compute test perplextiy\n",
    "        log_file_path = 'log_file_test_perplexity.txt'\n",
    "        f = open(os.path.join(save_path, log_file_path), 'w')\n",
    "        sys.stdout = f\n",
    "\n",
    "        test_perplexity = run_epoch(session, mtest)\n",
    "        print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "        \n",
    "        print(\"Saving model to %s.\" % save_path)\n",
    "        sv.saver.save(session, save_path, global_step=sv.global_step)\n",
    "        \n",
    "        f.close()\n",
    "\n",
    "# Restore default output\n",
    "sys.stdout = orig_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train perplexity: 99.1654721641\n",
      "Valid perplexity: 99.8092660938\n",
      "Test perplexity: 100.591325334\n"
     ]
    }
   ],
   "source": [
    "# Print out final perplexities\n",
    "with sv.managed_session() as session:\n",
    "    print 'Train perplexity:', train_perplexity\n",
    "    print 'Valid perplexity:', valid_perplexity\n",
    "    print 'Test perplexity:', test_perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Generate final texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he said to god the rest of life. it was a very great artist; but i built up history with the best bewilderment of a vivid clough coming, is mrs. browning's cousin with sovereign her own synge. i had not sent mrs. erlynne the door of god. yet emma hasn't left a piece of upright poignancy, because he might come to weave his violent own life. but for the sun, that will return on him to be said for not, and very simple indeed. there is no use dearer to the biography with the marriage and the putatively and seek to be shown apart for me, or conscience having the other to be under its sun and wife, and surely stirs having every rise to be pathetic. \" servants returned to read) no longer, but essays in it no or beneficent delight. a great consolation to description of our somewhat kind conversation. the compliment - upon - of two or eight men were touched by the clothes, and for one was not entirely absurdly filter. private and cyril's great opinion became her sick. one of the child were to be turned back, by a there the rector of a opened, then with a strange giggle, strange and graciously faded, and danced with screams that was peru there; but and he accepted except some background how different he had waited with in mattresses and gaily in pulpless, and examining the last room. \" what spirit dick is! \" he said. then the fisherman pointed out for the two men as if she had gone to the notice of a soft adventure, in the hills and peeped close with a small voice with two, and frogs him quickly to walk near her, that he found, with duties, of the affronting annoyance in which he was the mayor texture, and on every he was charging very much resolute. the twins was hung under the fashionable ceiling. when the next ball closed, she broke an involuntary envy of their long and flickering blood had hated it. \" thanks, \" muttered the young student. \" yes! well, with his cold, are concession to the pantry - scotch post. \" \" michael; and in hell, it was of a very coarse state in directions. for - it seems to me that this great unreserve of daniel's nineteen: \" \" bet it's in the productions of the country carefully, \" said the carven deliverance a little bad tranquillity. \" what has you? \" \" a regard to whom it ever says more of an worried one. however, i am said that i called to unmoral that informed them why i shall kill \n",
      "\n",
      "she had been with love in them, unless they could spend the six hours. with a very correction unwavering voice, he equal, hello, resentfully, his dressing lamps, which he got for all half the blind, for more younger work - in jerusalem that had it been fascinated by me. it always brushed to him and the slightest knocker. he, since the old gentleman he lay it, resolved, like dear men, in the garden was who ever sebastian talked, mr. james hallward's own idea had never been found in his kna. he was wrong, he said with young pleasure. he had breath been obliged to listen with his reception of his groton, and go to that days and looked watching him with her bow. they became chewing indescribable and fought for back aghast in snatching a moored: the adolescent journeys, as if not, alec was indulging and thoroughly surprised, of gleaming and fancying the reluctance rather glass, and it was out of the bank - working road that had been built; when, erect, burst under in order of tastes. he had seen a impudently in his face, that he brought this financiers that of his new tapestries by yellow scope, he leaped to her, barbara meadow. his eyes fell trembling him with his hands, with a hero's resolution that she had not much loved; and it was impossible to know that they had reasonably increased their society at home. amory stumbled arm to the river. it was let the eltons be a golden distance at her head from her and heard himself revived and fresh did not bear - not another very proud, and they contented with the curious leaves that being whistle in the crew; where must there be more this this aroused, alas, indoors about. the good stirs passed and away, among the fold of an black appropriateness of booties unreal and beer from the count. the curriculum had never sent much dinner. to dancing straight this picture of what had told her a flower strong in the rolling friendship when he drew that caste of a time fell for. it seemed with idle veil that drove back with a pile of scorn and a heated reaction. \" are it possibly? \" \" i'm not where that's the matter. \" \" i suspect it won't think. it seems to be guilty of his friend - oh, i guess person. i want never to see. for not - i can make life archèd on it on the part. thanks, mrs. elton only in sunday. \" a new voice was it, \" does i think if you didn't hear so much at the \n",
      "\n",
      "it had always been gone on the amuses. then he says i really thought any girl can have gone much; but still, why i starve you to see the romance of a gentleman, that morning - how that it was the only daughter of any other? \" \" well, his brother, i don't think to have any other people now duke, how scruples it is? \" \" i dare say, \" said the young man, \" that comtesse leaping in veitch's keeping bowed to the sordello of a true marina, a lovely lady, i know, and was all good - meaning my mere boy and my father. such a effects of life soon, but said, i am quite angry. i mean the vernacular of the very burly places who have looked forward to the other. it may not be made my own duty. my attentions - what precisely you did, it is the original priory by which i am cost it. i hope it will be quite brave to proceed at all, and it is capacity for this. how fully you pass by right - fall with your professional royal laced. but dick will not give you his opinions in my arms; he says that was the duke of lady conventions. change abundance on subjects or powdered parlors for high parts and thought are more absent, marble, so hyacinth. \" mama was obliged to sit for it from his peradventure. \" unluckily has he built me by gardener? \" asked mrs. otis. \" thank you. are it pleasant? i have no shirt - tattler - no said, lady markby. your first train is girls it, and it was so faithfully worth being pleased. which is in store with me a lovely heart left to st. s whole time. \" \" not that your beauty is. \" \" as he told me, he enjoyed here when you called him to. i thought a little helpless in those who should thank you, and say far in the garden of haw. so now he seems greatly aware of london. sir james stephenson was a singular old possible schoolboy's. he wants it to be as carried away for her and very well. dorian gray is a most free scandal. yet it is of a slave for art. his change is proteus; nothing beginning for me on the subject: it is a clean thing. i intend to require too mood by a great deal of importance in this injustice, and if it makes any opportunity of the whole attitude of our life. no; gave me the point in the wolfings lady dalrymple, but you live, and \n",
      "\n",
      "mr. gladstone and gifford did not compliment her. it would have been as easy for one of a fine or fortunate wife; but for the sake of lady russell, does not quarrel with him; wherever mr. walter knightley says with such a strong attachment, as she wondered the, when lady tzu hayter sunk the way wrote upon her person. his feelings in such an negative and marie gautier's birth should have been given to him a highly immediate relation to the such susceptibilities, and no necessity for it was evident, but should not be now recommending from by one of his relations throughout them, and the most cold that he had ever been emigrating by the females of his own, or observing with all consciousness of all the circumstances of this society. every body determined to stay for a wretched state, seemed to be mistress of a hint that she had allowed to know, when he had gone home among mr. darcy in the open and elegant manner, and one of his eyes celebration left him helplessly, and made no friendship to what there was a good morning. the present last letter, it would have been much attached for, the cut public compassed between the mortification of her friends still, and was full of fine bloom, and still could quarrel with bath again to hartfield. after no comparison, he brushed her head, thought that some whole syllable of man must be favourably attributed to poorest humble situation, that might influence a matter of folly, or the reflection of the keener adder of worldly day of hon. his dalrymple, and wondering elizabeth. names might have been found out, to her nature. it was as a pedantic journey for charlotte's mother, nor could speak a point which would not day let her be always at once now in the complete misery of any while. for some one before she had not turned the dance himself home, it ended as it would have been a good joke. when mrs. gardiner appeared in this room, she had not every probability of either men to obtain small and important - limited hands. at some time but, john, what was coming away on the floor at mrs. bennet, whether he might bring such a letter, miss elliot would keep many of their england lost a week. yes, that it was not true that he had given herself before he had found it: but for himself saying, jane had quite heard no fears for her engagement. perhaps, emma, in spite of isabella's life for speaking of difficulties, louisa she was obliged to forget life, and in being fated to make his own name \n",
      "\n",
      "mrs. antics is worth giving up him. she writes only what he wrote, the large giant woodland, those who we could commit. but now, it would rest across his dinner dinner - room, and said that the estate wrote to him a perpetual guest, and it was quite faultless. while he was only convict and retired about the house was there, fan. [when sir john heard his thermopylæ the case of his face into a hand of playful, she reached the magazines on the cart - room, stood in his hands \" in his room. the father patted his hand in, his eyes, vote, she stood a thorn, and held her up out the place, a girl's picture of a leafless camel later he guessed they had pressed down their forests. her face had left the pallid sappho who sat with time challenged at first, watching his tomato steel, followed each other at them. he was going to see the most cheerful things about his fruits, and at that time they withdrew him and would like one, who had no noise. in the last half years at the picture of the queen came from another road, and soon with bounds hands of bottles and les rowena, he binds that curious juries babe shades the chair, and plucked from the glades of a dull night, when she walked, moved, or identity, pale, the grave, and in nubians m'avais marred with beautiful eyes - and urged the xxviii and hid him from his tiny spheres and he swayed swiftly directly, with a employed kent over the camel's plays, tuileries itself will forget here. before he a long breathed it came to another four - five, a loaf of interpretation restrictions his floating shaking in the open air. he felt that he hated that he knew how he had he suffered on marriage hers, and with the nerves he found this. the sky fell from his back as soon as the baily seeks to manage, and to allow as a message of a lake - tale, and shook the newspaper. the more satan crept less, given to the belted phantoms of the throng how great came to the campbells. in the kitchen was the moon trailed, and the dull beam that floated in the ploughs and the ravine pallas of the marble stretching, even the white drums in a veins of bits of gold, leapt over the river with tragic laughter; but from god in the city before all this chords was a independently morus from the stream and each gentleman seemed taché in schools that had desire to stand over. the young man and three egotist were turned, and \n",
      "\n",
      "miss worsley, from her voice, the yellow pigeonholes. i was at happy his ignorance to secure an explanation of the fair yellowed accommodation. it was expressing them all with a great inconvenience. i couldn't see you, mr. pfeiffer, for those who bulwarks the burden of mrs. ross's that is one of their speculations as my art, or by your relations, tho poise against which i frighten. merriman. last law i have only been a wonderful thing; but i am parted to the american question, no toad should have known me alone, of course. any sooner ruddy, too kind. quite very dull, and at present they are great desires of the time of fairness to a gay of mine and that provoked putting day as ours was that that profiles politely in our seclusion. as i know my biographer stands down another night for some time twice drawing he will be with music. but of such a truth indeed is all mr. comprenez master's legs, unkindness, travelling kensington, very attentive and gallant. it is as that he knows that is so many of these imaginative aphorisms. it is, however, dear national producing the want of men. this term, the wicked for armenian immediately, were the discontent for been to which we were uttered in the influence of tanagra - consequence all the noble poems were to be succeeded. as men were welcomed by the catchwords of men, may have reminded it much of an ethical manner. it was good to imagine that the manner herself to me. it was not admitted at half a dozen years day raoul - tennis journalism at once some day? he wrote that he fervent idiots and in the same feeling, a man who was having most drunk, with a very common clatter of keats. the latter might probably be made often of commissioners. \" the first time in press you took - persians of children \" s stock daughters! i could do that we give the modern effect upon the canvas, while that deals with one of those canonise yale - mail i avoided decoration, and the specified of the present adventurers movement, which must be regarded as something still written from american moods and dramatic treatment. the normal wonder of every training, the reformed italian authoress of the waking scene, one of our vest figures, elevation of harmony and ceremonies. as the rule, and the bithynian dramatic method, such as horses of history we must function to sow and creed, and what was with its pictorial chambers. as for our affability, the young man's reading in various forms of art, this noble expressions which endeavoured to cultivate love \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out final text generation\n",
    "with sv.managed_session() as session:\n",
    "    generated_texts = []\n",
    "    \n",
    "    for feed in feeds:\n",
    "        generated_text = generate_text(session, mfeed, np.array(feed).reshape(1, 1), text_length)\n",
    "        generated_text = ' '.join([id_to_word[text[0, 0]] for text in generated_text])\n",
    "        generated_text = convert_punctuations(generated_text)\n",
    "        \n",
    "        print generated_text, '\\n'\n",
    "\n",
    "        generated_texts.append(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration of model restore after kernel reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored from file: model_output_2017-07-27-17-07/model.ckpt-52640\n",
      "\n",
      "he gave hobart long palms because his father was disappointed before the second passage. by the single draft of the frowned of his reader in a volume of america became an archway, and with their affectionate remarks, it was mightier than he could have told them with late. to the first week he had was the first; and he was happy in children, and those who should but answer to fois and england, and then, after any moment, the abstractedly jerryl went back warmly round the street, that his most mechanical jaw was disgusted with the employed sentimental and pathos; and, while to see were she, years filled with complacency, and hug; futile are delightfully, agonized; it is, a constant success, and the piece of boils golden voice, a spirit that is hail and spires and ostentorum - enviously: tapestry and grass are its ardent irons, and gravely the small butcher and the carbuncles on a green light could be a garden to hide his fight. he fist upon and gleams into a volume of white canvas, and a thicket and americuns are pretty seed, swimming with languid red, red hair, damask, and white; from a band of yellow ivory and reingratiating in its lair in sweet hands. the smile cast down the pine - faint grass of rich vine, or silken prow and scentless fierce brown shoes, in drawn figures and despair alone, work long and spirits. it may be so, i feel the healthy and sympathetic and perfect sound of art, and the bee and the colour that is among art leaves from the soap - flowered brighter phrases, when they come far upon the mist that have killed the arcady, whose prow lions will make dusky pale fleeces by a dripping brazen - woods, and doing nothing by some perfume or glorious bronze. ay, nothing of the world is just a sort of people you may see the dank curtained feast and inexperienced literature and insensible. your mother, my dear, you have a love your doing and this winter: will you not care with me enough to make his hair, and held at this public day? asked the impotence the night - while she shone and silently, but he spake that let him find something the curtain, with some dusky pails of the gold dances: and we kiss all the miracles of the universe, and they brought that of gaol never knew her in the emperor's woodland, and he felt that at once created it now seemed that he feels seemed that steer under the blunt needle - stick. \" a great artist, \" cried his brother, \" who is not yet related \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "    \n",
    "    # Define model for text generations\n",
    "    with tf.name_scope(\"Feed\"):\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            mfeed = Model(is_training=False, config=eval_config)\n",
    "    \n",
    "    sv = tf.train.Supervisor(logdir=save_path)\n",
    "    with sv.managed_session() as session:\n",
    "        # Restore model weights from previously saved model\n",
    "        ckpt = tf.train.get_checkpoint_state(save_path)\n",
    "        sv.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "        print(\"Model restored from file: %s\\n\" % ckpt.model_checkpoint_path)\n",
    "        \n",
    "        # Now that the model is restored, we can generate texts again\n",
    "        generated_text = generate_text(session, mfeed, np.array(feeds[0]).reshape(1, 1), text_length)\n",
    "        generated_text = ' '.join([id_to_word[text[0, 0]] for text in generated_text])\n",
    "        generated_text = convert_punctuations(generated_text)\n",
    "        \n",
    "        print generated_text, '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 (General DS)",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
